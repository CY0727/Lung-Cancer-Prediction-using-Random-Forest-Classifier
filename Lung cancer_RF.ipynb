{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7e969-a482-49b9-ad2c-72f1fb13835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, f1_score, recall_score, precision_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "# Data preprocessing function\n",
    "def preprocess_data(data_path, group_mapping):\n",
    "    try:\n",
    "        data = pd.read_excel(data_path)\n",
    "        data['Group'] = data['Group'].map(group_mapping)\n",
    "        X = data.drop('Group', axis=1)\n",
    "        y = data['Group']\n",
    "        X.columns = X.columns.astype(str)\n",
    "        \n",
    "        # Apply MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "        \n",
    "        return X_scaled, y\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Performance metrics calculation function\n",
    "def calculate_performance_metrics(y_true, y_pred):\n",
    "    try:\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        return precision, recall, f1, specificity, mcc, cm, accuracy\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_performance_metrics: {e}\")\n",
    "        return None, None, None, None, None, None, None\n",
    "\n",
    "# Calculate the best threshold\n",
    "def calculate_best_threshold(fpr, tpr, thresholds):\n",
    "    try:\n",
    "        J = tpr - fpr\n",
    "        ix = np.argmax(J)\n",
    "        best_threshold = thresholds[ix]\n",
    "        \n",
    "        return best_threshold, tpr[ix], fpr[ix], ix\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_best_threshold: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# OOB accuracy and 95% confidence interval calculation\n",
    "def calculate_oob_ci(oob_score, n):\n",
    "    try:\n",
    "        oob_se = np.sqrt(oob_score * (1 - oob_score) / n)\n",
    "        z = norm.ppf(0.975)  # 1.96 for 95% CI\n",
    "        ci_lower = oob_score - z * oob_se\n",
    "        ci_upper = oob_score + z * oob_se\n",
    "        \n",
    "        return ci_lower, ci_upper\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_oob_ci: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Main function to analyze the data and plot the results\n",
    "def analyze_data(data_path, label, best_params):\n",
    "    # Load and preprocess data\n",
    "    X_train, y_train = preprocess_data(data_path, {'NT': 0, 'T': 1})\n",
    "    if X_train is None or y_train is None:\n",
    "        print(f\"Failed to preprocess data for {label}\")\n",
    "        return None, None\n",
    "\n",
    "    # Train model and get feature importance\n",
    "    rf_model = RandomForestClassifier(**best_params)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    features = X_train.columns\n",
    "    sorted_features = [feature for _, feature in sorted(zip(feature_importances, features), reverse=True)]\n",
    "    top_5_feature_names = sorted_features[:5]\n",
    "    print(f'{label} top_5_feature_names:')\n",
    "    print(top_5_feature_names)\n",
    "\n",
    "    # Calculate performance metrics using original data\n",
    "    oob_probabilities = rf_model.oob_decision_function_[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, oob_probabilities)\n",
    "    auc_score = roc_auc_score(y_train, oob_probabilities)\n",
    "\n",
    "    # Calculate the best threshold\n",
    "    best_threshold, best_tpr, best_fpr, ix = calculate_best_threshold(fpr, tpr, thresholds)\n",
    "    if best_threshold is None:\n",
    "        print(f\"Failed to calculate best threshold for {label}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f'{label} Original cut-off value: {best_threshold}')\n",
    "    print(f'{label} Original TPR: {best_tpr}, FPR: {best_fpr}')\n",
    "\n",
    "    # Performance metrics\n",
    "    oob_predictions = (oob_probabilities > 0.5).astype(int)\n",
    "    precision, recall, f1, specificity, mcc, cm, accuracy = calculate_performance_metrics(y_train, oob_predictions)\n",
    "    if precision is None:\n",
    "        print(f\"Failed to calculate performance metrics for {label}\")\n",
    "        return None, None\n",
    "\n",
    "    # OOB accuracy and 95% confidence interval\n",
    "    oob_accuracy = rf_model.oob_score_\n",
    "    ci_lower, ci_upper = calculate_oob_ci(oob_accuracy, len(y_train))\n",
    "    if ci_lower is None:\n",
    "        print(f\"Failed to calculate OOB CI for {label}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"{label} Original OOB Accuracy: {oob_accuracy:.4f}\")\n",
    "    print(f\"{label} Original 95% CI for Accuracy: ({ci_lower:.4f}, {ci_upper:.4f})\")\n",
    "    print(f\"{label} Original Precision: {precision:.4f}, Recall (Sensitivity): {recall:.4f}, F1 Score: {f1:.4f}, Specificity: {specificity:.4f}, Accuracy: {accuracy:.4f}, MCC: {mcc:.4f}\")\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1 Score', 'Specificity', 'Accuracy', 'MCC']\n",
    "    values_train = [precision, recall, f1, specificity, accuracy, mcc]\n",
    "\n",
    "    original_results = (fpr, tpr, auc_score, values_train, metrics, label, best_threshold, ix, oob_accuracy, ci_lower, ci_upper, cm)\n",
    "\n",
    "    # Function to evaluate model using OOB validation\n",
    "    def evaluate_model_oob(feature_subset):\n",
    "        X_train_selected = X_train[list(feature_subset)]\n",
    "        model = RandomForestClassifier(**best_params)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        oob_probabilities = model.oob_decision_function_[:, 1]\n",
    "        oob_auc = roc_auc_score(y_train, oob_probabilities)\n",
    "        return oob_auc, feature_subset\n",
    "\n",
    "    # Generate feature combinations using itertools\n",
    "    all_feature_combinations = []\n",
    "    for i in range(1, len(top_5_feature_names) + 1):\n",
    "        combinations = list(itertools.combinations(top_5_feature_names, i))\n",
    "        all_feature_combinations.extend(combinations)\n",
    "\n",
    "    # Perform parallel computation using Parallel and delayed\n",
    "    results = Parallel(n_jobs=-1)(delayed(evaluate_model_oob)(combo) for combo in all_feature_combinations)\n",
    "\n",
    "    # Find the best combination\n",
    "    best_result = max(results, key=lambda x: x[0])\n",
    "    best_auc, best_combination = best_result\n",
    "\n",
    "    print(f'{label} Best AUC: {best_auc:.4f}')\n",
    "    print(f'{label} Best feature combination:', best_combination)\n",
    "\n",
    "    # Plot ROC curve for the best combination\n",
    "    X_train_best = X_train[list(best_combination)]\n",
    "    model_best = RandomForestClassifier(**best_params)\n",
    "    model_best.fit(X_train_best, y_train)\n",
    "    oob_probabilities_best = model_best.oob_decision_function_[:, 1]\n",
    "    fpr_best, tpr_best, thresholds_best = roc_curve(y_train, oob_probabilities_best)\n",
    "    auc_score_best = roc_auc_score(y_train, oob_probabilities_best)\n",
    "\n",
    "    # Calculate the best threshold for the best combination\n",
    "    best_threshold_best, best_tpr_best, best_fpr_best, ix_best = calculate_best_threshold(fpr_best, tpr_best, thresholds_best)\n",
    "    if best_threshold_best is None:\n",
    "        print(f\"Failed to calculate best threshold for {label} Best\")\n",
    "        return original_results, None\n",
    "\n",
    "    print(f'{label} Best cut-off value: {best_threshold_best}')\n",
    "    print(f'{label} Best TPR: {best_tpr_best}, FPR: {best_fpr_best}')\n",
    "\n",
    "    best_results = (fpr_best, tpr_best, auc_score_best, best_threshold_best, ix_best)\n",
    "\n",
    "    return original_results, best_results\n",
    "\n",
    "# Define best parameter combinations\n",
    "best_params_entire_dataset = {\n",
    "    'bootstrap': True,\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 4,\n",
    "    'max_features': None,\n",
    "    'min_samples_leaf': 4,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 500,\n",
    "    'random_state': 42, \n",
    "    'oob_score': True\n",
    "}\n",
    "\n",
    "best_params_primary_lc = {\n",
    "    'bootstrap': True,\n",
    "    'criterion': 'entropy',\n",
    "    'max_depth': 4,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 4,\n",
    "    'min_samples_split': 10, \n",
    "    'n_estimators': 500,\n",
    "    'random_state': 42,\n",
    "    'oob_score': True\n",
    "}\n",
    "\n",
    "# Analyze two datasets\n",
    "data_path1 = 'Primary LC dataset.xlsx'\n",
    "data_path2 = 'Entire dataset.xlsx'\n",
    "\n",
    "results_primary, best_results_primary = analyze_data(data_path1, 'Primary LC', best_params_primary_lc)\n",
    "results_entire, best_results_entire = analyze_data(data_path2, 'Entire Dataset', best_params_entire_dataset)\n",
    "\n",
    "# Plot Confusion Matrix for Primary LC\n",
    "if results_primary:\n",
    "    fpr_primary, tpr_primary, auc_primary, values_primary, metrics_primary, label_primary, best_threshold_primary, ix_primary, oob_accuracy_primary, ci_lower_primary, ci_upper_primary, cm_primary = results_primary\n",
    "    \n",
    "    plt.figure(figsize=(8, 8), dpi=300)\n",
    "    sns.heatmap(cm_primary, annot=True, fmt='d', cmap='Oranges', xticklabels=['Non-Tumor', 'Tumor'], yticklabels=['Non-Tumor', 'Tumor'], cbar=True)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix for Primary LC')\n",
    "    plt.show()\n",
    "\n",
    "# Plot Confusion Matrix for Entire Dataset\n",
    "if results_entire:\n",
    "    fpr_entire, tpr_entire, auc_entire, values_entire, metrics_entire, label_entire, best_threshold_entire, ix_entire, oob_accuracy_entire, ci_lower_entire, ci_upper_entire, cm_entire = results_entire\n",
    "    \n",
    "    plt.figure(figsize=(8, 8), dpi=300)\n",
    "    sns.heatmap(cm_entire, annot=True, fmt='d', cmap='Greens', xticklabels=['Non-Tumor', 'Tumor'], yticklabels=['Non-Tumor', 'Tumor'], cbar=True)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix for Entire Dataset')\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curve and model performance for Primary LC\n",
    "if results_primary and best_results_primary:\n",
    "    plt.figure(figsize=(8, 8), dpi=300)\n",
    "    plt.plot(fpr_primary, tpr_primary, color='orange', lw=2, label=f'Primary LC (AUC = {auc_primary:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.scatter(fpr_primary[ix_primary], tpr_primary[ix_primary], marker='o', color='red', label=f'Primary LC Cut-off value: {best_threshold_primary:.2f}')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Primary LC ROC Curve', fontsize=16, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), dpi=300)\n",
    "    x = range(len(metrics_primary))\n",
    "\n",
    "    ax.plot(x, values_primary, marker='o', linestyle='-', color='orange', label='Primary LC')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_primary, fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.set_title('Primary LC Model Performance', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend(loc='lower right', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot combined ROC curve\n",
    "if results_primary and results_entire:\n",
    "    plt.figure(figsize=(8, 8), dpi=300)\n",
    "    plt.plot(fpr_primary, tpr_primary, color='orange', lw=2, label=f'Primary LC (AUC = {auc_primary:.2f})')\n",
    "    plt.plot(fpr_entire, tpr_entire, color='green', lw=2, linestyle='-.', label=f'Entire Dataset (AUC = {auc_entire:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.scatter(fpr_primary[ix_primary], tpr_primary[ix_primary], marker='o', color='red', label=f'Primary LC Cut-off value: {best_threshold_primary:.2f}')\n",
    "    plt.scatter(fpr_entire[ix_entire], tpr_entire[ix_entire], marker='x', color='red', label=f'Entire Dataset Cut-off value: {best_threshold_entire:.2f}')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Combined ROC Curve', fontsize=16, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparison of model performance for Primary LC and Entire Dataset\n",
    "if results_primary and results_entire:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), dpi=300)\n",
    "    x = range(len(metrics_primary))\n",
    "\n",
    "    ax.plot(x, values_primary, marker='o', linestyle='-', color='orange', label='Primary LC')\n",
    "    ax.plot(x, values_entire, marker='x', linestyle='-.', color='green', label='Entire Dataset')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_primary, fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.set_title('Primary LC vs Entire Dataset Model Performance', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend(loc='lower right', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
